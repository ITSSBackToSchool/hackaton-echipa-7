import os
import requests
from dotenv import load_dotenv

# ------------------------------------------------------------
# ğŸ”¹ ÃncÄƒrcare cheie API din fiÈ™ierul .env
# ------------------------------------------------------------
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
ENV_ENDPOINT = os.getenv("GEMINI_ENDPOINT")  # ex.: v1 / v1beta
ENV_MODEL = os.getenv("GEMINI_MODEL")        # ex.: gemini-2.5-flash
CACHE_PATH = os.path.join(os.path.dirname(__file__), ".gemini_model_cache.json")

if not API_KEY:
    raise Exception("âŒ Nu s-a gÄƒsit cheia GOOGLE_API_KEY Ã®n fiÈ™ierul .env")

# --- Global system instruction (persona) ---
BASE_SYSTEM_INSTRUCTION = """
EÈ™ti Chef Asistent (ChefGPT), un agent culinar prietenos È™i practic.

Reguli generale:
- RÄƒspunde Ã®n romÃ¢nÄƒ, clar È™i concis; preferÄƒ bullet points È™i paÈ™i numerotaÈ›i.
- ÃntreabÄƒ doar cÃ¢nd e necesar (alergii, timp, cÃ¢È›i oameni mÄƒnÃ¢ncÄƒ).
- SugereazÄƒ 1â€“3 reÈ›ete relevante, nu te opri niciodatÄƒ doar la listarea ingredientelor.
- Include timp total, dificultate, paÈ™i (1..N), È™i opÈ›ional listÄƒ scurtÄƒ de cumpÄƒrÄƒturi/Ã®nlocuiri.

Scenarii:
- Cu inventar (din frigider): foloseÈ™te DOAR ingredientele date ca bazÄƒ; marcheazÄƒ clar ce lipseÈ™te È™i oferÄƒ alternative.
- FÄƒrÄƒ inventar (cerere tip â€cum fac X?â€ sau â€idee de cinÄƒ rapidÄƒâ€): rÄƒspunde direct cu reÈ›eta/ideile cerute, fÄƒrÄƒ a cere inventarul.
"""

# ------------------------------------------------------------
# ğŸ”¹ FuncÈ›ie care testeazÄƒ automat ce endpoint È™i modele merg
# ------------------------------------------------------------
def detect_working_model():
    """
    SelecteazÄƒ automat modelul cu cea mai micÄƒ latenÈ›Äƒ dintre candidaÈ›i,
    pe oricare dintre endpoint-urile suportate. MÄƒsoarÄƒ timpul efectiv al
    unui request minimal È™i alege cel mai rapid care rÄƒspunde 200.
    """
    import time
    endpoints = ["v1beta", "v1"]  # v1beta e adesea mai liber
    # PreferÄƒm modelele FLASH (mai rapide, mai ieftine). Scoatem PRO din autodetect ca sÄƒ evitÄƒm 404/permisiuni.
    candidates = [
        "gemini-2.5-flash",
        "gemini-2.0-flash",
        "gemini-1.5-flash",
    ]

    test_payload = {"contents": [{"parts": [{"text": "ping"}]}]}
    headers = {"Content-Type": "application/json"}

    best = None  # (elapsed, endpoint, model)
    for endpoint in endpoints:
        for model in candidates:
            try:
                url = f"https://generativelanguage.googleapis.com/{endpoint}/models/{model}:generateContent?key={API_KEY}"
                t0 = time.perf_counter()
                resp = requests.post(url, headers=headers, json=test_payload, timeout=8)
                elapsed = time.perf_counter() - t0
                if resp.status_code == 200:
                    if best is None or elapsed < best[0]:
                        best = (elapsed, endpoint, model)
                else:
                    print(f"â†ªï¸ {endpoint}/{model} status {resp.status_code}")
            except Exception as e:
                print(f"âš ï¸ Eroare la testarea {endpoint}/{model}: {e}")

    if best:
        print(f"âœ… Aleg cel mai rapid: '{best[2]}' pe '{best[1]}' (â‰ˆ{best[0]:.2f}s)")
        return best[1], best[2]
    raise Exception("âŒ Nu am putut iniÈ›ializa niciun model Gemini. VerificÄƒ GOOGLE_API_KEY È™i cotele.")

# ------------------------------------------------------------
# ğŸ”¹ SelecteazÄƒ automat endpointul È™i modelul compatibil
#    Permite override din .env (GEMINI_ENDPOINT, GEMINI_MODEL)
# ------------------------------------------------------------
def _load_cached_model():
    try:
        import json
        if os.path.exists(CACHE_PATH):
            with open(CACHE_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                ep, md = data.get('endpoint'), data.get('model')
                if ep and md:
                    print(f"â„¹ï¸ Folosesc modelul din cache: '{md}' pe '{ep}'")
                    return ep, md
    except Exception:
        pass
    return None, None

def _save_cached_model(endpoint: str, model: str):
    try:
        import json
        with open(CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump({"endpoint": endpoint, "model": model}, f)
    except Exception:
        pass

def _select_endpoint_and_model():
    # 1) .env override
    if ENV_ENDPOINT and ENV_MODEL:
        print(f"âœ… Folosesc modelul (din .env) '{ENV_MODEL}' pe endpoint '{ENV_ENDPOINT}'")
        return ENV_ENDPOINT, ENV_MODEL
    # 2) cache de la ultima rulare reuÈ™itÄƒ
    ep, md = _load_cached_model()
    if ep and md:
        return ep, md
    # 3) autodetect
    return detect_working_model()

ENDPOINT, MODEL = _select_endpoint_and_model()

# ------------------------------------------------------------
# ğŸ”¹ Helper comun: trimite prompt cÄƒtre Gemini cu retry + fallback modele
# ------------------------------------------------------------
def _generate_with_retries(prompt: str, timeout_s: int = 30) -> str:
    import time

    # Ordinea candidaÈ›ilor: modelul curent, apoi alte variante FLASH
    candidates = [m for m in [MODEL, "gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-flash"] if m]
    headers = {"Content-Type": "application/json"}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    for model_name in candidates:
        url = f"https://generativelanguage.googleapis.com/{ENDPOINT}/models/{model_name}:generateContent?key={API_KEY}"
        for attempt in range(1, 3):  # douÄƒ Ã®ncercÄƒri/model
            resp = requests.post(url, headers=headers, json=payload, timeout=timeout_s)
            if resp.status_code == 200:
                data = resp.json()
                try:
                    # cacheazÄƒ modelul reuÈ™it pentru a fi preferat la startup
                    _save_cached_model(ENDPOINT, model_name)
                    return data["candidates"][0]["content"]["parts"][0]["text"]
                except KeyError:
                    raise Exception(f"âš ï¸ Format neaÈ™teptat al rÄƒspunsului API: {data}")
            if resp.status_code in (429, 503):
                # backoff scurt È™i reÃ®ncercare
                time.sleep(2 * attempt)
                continue
            # alte erori â€“ propagÄƒ imediat
            raise Exception(f"API error {_current_func_name()}: {resp.status_code} - {resp.text}")
        # trecem la urmÄƒtorul model
    raise Exception("Toate modelele sunt ocupate momentan (429/503). ÃncearcÄƒ din nou mai tÃ¢rziu.")

def _current_func_name() -> str:
    # mic utilitar pentru mesaje de eroare
    import inspect
    for frame in inspect.stack():
        if frame.function.startswith('generate_'):
            return frame.function
    return 'generate'

# ------------------------------------------------------------
# ğŸ”¹ FuncÈ›ia principalÄƒ de generare reÈ›ete (moÈ™tenitÄƒ)
# ------------------------------------------------------------
def generate_recipes(ingredients):
    """
    GenereazÄƒ reÈ›ete creative folosind Gemini 2.5 / Flash.
    DacÄƒ modelul principal e supraÃ®ncÄƒrcat (503), reÃ®ncearcÄƒ automat.
    """
    prompt = f"""
    EÈ™ti ChefGPT, un asistent culinar inteligent.
    AvÃ¢nd urmÄƒtoarele ingrediente: {', '.join(ingredients)},
    creeazÄƒ 3 reÈ›ete creative care sÄƒ includÄƒ:
    - Titlu È™i descriere
    - Lista completÄƒ de ingrediente
    - PaÈ™i de preparare numerotaÈ›i
    - Timp de preparare È™i calorii
    - Sugestii de servire
    RÄƒspunde Ã®n limba romÃ¢nÄƒ, frumos formatat Ã®n Markdown.
    """

    url = f"https://generativelanguage.googleapis.com/{ENDPOINT}/models/{MODEL}:generateContent?key={API_KEY}"
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {"Content-Type": "application/json"}

    # ğŸ” ReÃ®ncercare automatÄƒ de 3 ori, cu fallback la model mai mic
    attempts = 0
    max_attempts = 3
    fallback_model = "gemini-2.0-flash"

    while attempts < max_attempts:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        data = response.json()

        # âœ… Succes
        if response.status_code == 200:
            try:
                return data["candidates"][0]["content"]["parts"][0]["text"]
            except KeyError:
                raise Exception(f"âš ï¸ Format neaÈ™teptat al rÄƒspunsului API: {data}")

        # âš ï¸ Model supraÃ®ncÄƒrcat
        elif response.status_code == 503:
            attempts += 1
            print(f"âš ï¸ Modelul {MODEL} este supraÃ®ncÄƒrcat ({attempts}/{max_attempts})... reÃ®ncerc Ã®n 5 secunde.")
            import time
            time.sleep(5)
            if attempts == max_attempts:
                print("â³ Trec pe modelul de rezervÄƒ:", fallback_model)
                url = f"https://generativelanguage.googleapis.com/{ENDPOINT}/models/{fallback_model}:generateContent?key={API_KEY}"
                continue

        # âŒ AltÄƒ eroare API
        else:
            raise Exception(f"API error generate_recipes: {response.status_code} - {response.text}")

    raise Exception("âŒ Toate Ã®ncercÄƒrile au eÈ™uat. ÃncearcÄƒ mai tÃ¢rziu.")

# ------------------------------------------------------------
# ğŸ”¹ FuncÈ›ie nouÄƒ: idei adaptate cu frigider + reÈ›ete din DB + masa vizatÄƒ
# ------------------------------------------------------------
def generate_meal_suggestions(ingredients, user_recipes=None, meal_hint=None):
    """
    GenereazÄƒ sugestii/meniuri folosind atÃ¢t ingredientele din frigider, cÃ¢t È™i reÈ›etele utilizatorului (DB),
    avÃ¢nd opÈ›ional o masÄƒ vizatÄƒ (mic dejun / prÃ¢nz / cinÄƒ / snack).
    Prompt extins pentru a obÈ›ine rezultate cÃ¢t mai generative È™i utile.
    """
    user_recipes = user_recipes or []
    short_recipes = []
    for r in user_recipes:
        name = r.get('name', 'ReÈ›etÄƒ')
        ingreds = r.get('ingredients', [])
        short_recipes.append(f"- {name}: {', '.join([str(x) for x in ingreds])}")

    meal_line = f"Masa vizatÄƒ: {meal_hint}." if meal_hint else "(masa la alegere)"
    prompt = f"""
    {BASE_SYSTEM_INSTRUCTION}

    Context:
    - Ingrediente disponibile (frigider): {', '.join(ingredients) if ingredients else 'â€”'}
    - ReÈ›ete ale utilizatorului (din baza de date):
      {chr(10).join(short_recipes) if short_recipes else '- (niciuna)'}
    - {meal_line}

    CerinÈ›e pentru rÄƒspuns (Markdown, concis, executabil):
    1) Propune 1â€“3 reÈ›ete FEZABILE pe baza inventarului, nu te opri la listÄƒ de ingrediente.
    2) Pentru fiecare reÈ›etÄƒ oferÄƒ:
       - Titlu
       - Timp total | Dificultate
       - Ingrediente folosite din frigider
       - Ingrediente lipsÄƒ/opÈ›ionale (cu Ã®nlocuiri posibile)
       - PaÈ™i 1..N clari (max 6)
    3) DacÄƒ vezi potriviri cu reÈ›etele utilizatorului, menÈ›ioneazÄƒ â€Compatibil cu reÈ›eta ta: <nume>â€.
    4) Ãncheie Ã®ntrebÃ¢nd: â€Alege o reÈ›etÄƒ (1â€“3) ca sÄƒ-È›i dau cantitÄƒÈ›ile exacte È™i paÈ™ii detaliaÈ›i.â€
    """

    return _generate_with_retries(prompt)

# ------------------------------------------------------------
# ğŸ”¹ FÄƒrÄƒ inventar: reÈ›ete/idei creative direct din Ã®ntrebare
# ------------------------------------------------------------
def generate_creative_recipes(user_query: str, k: int = 2):
    """
    GenereazÄƒ idei/retete plecÃ¢nd DOAR de la cererea utilizatorului, fÄƒrÄƒ a apela inventarul.
    """
    prompt = f"""
    {BASE_SYSTEM_INSTRUCTION}

    Cerere utilizator: "{user_query}"

    OferÄƒ {k} reÈ›ete/idei relevante. Pentru fiecare:
    - Titlu
    - Timp total | Dificultate
    - Ingrediente
    - PaÈ™i 1..N (clari, max 7)
    - VariaÈ›ii/Ã®nlocuiri dacÄƒ e util
    """

    return _generate_with_retries(prompt)

# ------------------------------------------------------------
# ğŸ”¹ Chat-only generic text (Gemini prompt minimalist)
# ------------------------------------------------------------
def generate_chat_reply(message):
    """
    Companion chat: rÄƒspunde liber la orice subiect. DacÄƒ utilizatorul aduce mÃ¢ncarea Ã®n discuÈ›ie,
    oferÄƒ idei, dar nu forÈ›a subiectul. Ton cald, empatic, scurt, cu eventualÄƒ Ã®ntrebare de follow-up.
    """
    prompt = f"""
    {BASE_SYSTEM_INSTRUCTION}

    ConversaÈ›ie liberÄƒ. RÄƒspunde la mesajul de mai jos ca un companion AI:

    â€{message}â€

    Stil: rÄƒspuns scurt-mediu, Ã®n romÃ¢nÄƒ, o Ã®ntrebare de follow-up cÃ¢nd are sens. Nu forÈ›a subiectul culinar.
    """
    return _generate_with_retries(prompt)


# ------------------------------------------------------------
# ğŸ”¹ Test local (doar dacÄƒ rulezi acest fiÈ™ier direct)
# ------------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ³ Testare ChefGPT...\n")
    rezultat = generate_recipes(["cartofi", "ouÄƒ", "brÃ¢nzÄƒ"])
    print(rezultat)
